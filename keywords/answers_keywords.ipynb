{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity & Keyword Extraction\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2022/03/keyword-extraction-methods-from-documents-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "\n",
    "import keybert\n",
    "import litellm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytextrank  # noqa: F401\n",
    "import spacy\n",
    "import spacy.cli\n",
    "import spacy.displacy\n",
    "import spacy.tokens\n",
    "# import spacy_transformers  # noqa: F401\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-27 19:25:48.760\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[34m\u001b[1mOPENAI_API_KEY='sk-'...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42**3)\n",
    "pd.options.display.max_rows = 50\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "logger.debug(f\"OPENAI_API_KEY='{OPENAI_API_KEY[:3]}'...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-27 19:25:54.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_spacy_model\u001b[0m:\u001b[36m4\u001b[0m - \u001b[34m\u001b[1mloaded spacy model name='en_core_web_sm' path='/Users/nameless/dev/proj/ml-practice-time/.venv/lib/python3.12/site-packages/en_core_web_sm/en_core_web_sm-3.7.1'\u001b[0m\n",
      "\u001b[32m2024-07-27 19:25:55.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_spacy_model\u001b[0m:\u001b[36m4\u001b[0m - \u001b[34m\u001b[1mloaded spacy model name='en_core_web_md' path='/Users/nameless/dev/proj/ml-practice-time/.venv/lib/python3.12/site-packages/en_core_web_md/en_core_web_md-3.7.1'\u001b[0m\n",
      "\u001b[32m2024-07-27 19:25:56.183\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_spacy_model\u001b[0m:\u001b[36m4\u001b[0m - \u001b[34m\u001b[1mloaded spacy model name='en_core_web_lg' path='/Users/nameless/dev/proj/ml-practice-time/.venv/lib/python3.12/site-packages/en_core_web_lg/en_core_web_lg-3.7.1'\u001b[0m\n",
      "\u001b[32m2024-07-27 19:25:57.116\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_spacy_model\u001b[0m:\u001b[36m4\u001b[0m - \u001b[34m\u001b[1mloaded spacy model name='en_core_web_trf' path='/Users/nameless/dev/proj/ml-practice-time/.venv/lib/python3.12/site-packages/en_core_web_trf/en_core_web_trf-3.7.3'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def load_spacy_model(spacy_model_name: str):\n",
    "    try:\n",
    "        spacy_nlp = spacy.load(spacy_model_name)\n",
    "        logger.debug(f\"loaded spacy model name='{spacy_model_name}' path='{spacy_nlp.path}'\")\n",
    "    except Exception as e:\n",
    "        logger.debug(str(e))\n",
    "        spacy.cli.download(spacy_model_name)\n",
    "        logger.debug(f\"downloaded spacy model from web name='{spacy_model_name}'\")\n",
    "        spacy_nlp = spacy.load(spacy_model_name)\n",
    "        logger.debug(f\"loaded spacy model name='{spacy_model_name}' path='{spacy_nlp.path}'\")\n",
    "\n",
    "\n",
    "load_spacy_model(\"en_core_web_sm\")\n",
    "load_spacy_model(\"en_core_web_md\")\n",
    "load_spacy_model(\"en_core_web_lg\")\n",
    "# load_spacy_model(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-27 19:25:58.060\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[34m\u001b[1mloaded news articles n=33\u001b[0m\n",
      "\u001b[32m2024-07-27 19:25:58.060\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[34m\u001b[1mJSON fields: ['title', 'description', 'partial_text', 'url', 'published_at', 'media_source_name', 'media_source_url', 'listing_query', 'listing_source', 'full_text', 'tags', 'nltk_summary', 'nltk_keywords']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5 articles: [8, 22, 29, 31, 32] ---\n",
      "Index: 8\n",
      "Date: 2024-07-25T14:56:21+00:00\n",
      "Title: Moldova's finance minister accepts new job at central bank\n",
      "Text: Moldova’s finance minister accepts new job at central bank\n",
      "\n",
      "1 minute\n",
      "\n",
      "(Reuters) – Moldovan Finance M...\n",
      "\n",
      "Index: 22\n",
      "Date: 2024-07-26T04:00:00+00:00\n",
      "Title: Gonzales man accused of killing wife has history of domestic abuse, murder allegations dating back to 1990s\n",
      "Text: Gonzales man accused of killing wife has history of domestic abuse, murder allegations dating back t...\n",
      "\n",
      "Index: 29\n",
      "Date: 2024-07-26T01:02:03+00:00\n",
      "Title: Deadpool & Wolverine’s Mid- & Post-Credits Scenes, Explained\n",
      "Text: Photo: Jay Maidment/20th Century Studios/MARVEL\n",
      "\n",
      "Warning: Spoilers ahead for Deadpool & Wolverine.\n",
      "\n",
      "...\n",
      "\n",
      "Index: 31\n",
      "Date: 2024-07-25T19:00:00+00:00\n",
      "Title: Blake Lively reacts to Taylor Swift’s cheeky comment about Ryan Reynolds\n",
      "Text: Taylor Swift also revealed that she’s godmother to Blake Lively and Ryan Reynolds’ kids\n",
      "\n",
      "Blake Livel...\n",
      "\n",
      "Index: 32\n",
      "Date: 2024-07-25T15:00:18+00:00\n",
      "Title: Chinese Shopkeeper Shows AMD Ryzen 9000 \"Zen 5\" Desktop CPU Prices, Lower Than Ryzen 7000 Series\n",
      "Text: A Chinese shopkeeper based in Bejing has a really weird yet funny way of showing off AMD Ryzen 9000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"news_articles.json\", \"rt\") as f:\n",
    "    news_articles: list[dict] = json.load(f)\n",
    "\n",
    "logger.debug(f\"loaded news articles n={len(news_articles)}\")\n",
    "logger.debug(f\"JSON fields: {list(news_articles[0].keys())}\")\n",
    "print()\n",
    "\n",
    "indices = sorted(list(np.random.permutation(len(news_articles))[:5]))\n",
    "print(f\"--- 5 articles: {indices} ---\")\n",
    "for i in indices:\n",
    "    article = news_articles[i]\n",
    "    print(f'Index: {i}')\n",
    "    print(f'Date: {article[\"published_at\"]}')\n",
    "    print(f'Title: {article[\"title\"]}')\n",
    "    print(f'Text: {article[\"full_text\"][:100]}...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction\n",
    "\n",
    "Let's extract entities from text: organizations, persons, and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Chinese Shopkeeper Shows AMD Ryzen 9000 \"Zen 5\" Desktop CPU Prices, Lower Than Ryzen 7000 Series\n",
      "Date: 2024-07-25T15:00:18+00:00\n",
      "Source: Wccftech\n",
      "\n",
      "A Chinese shopkeeper based in Bejing has a really weird yet funny way of showing off AMD Ryzen 9000 \"Zen 5\" Desktop CPU prices which appear to be lower than the Ryzen 7000 MSRP's for China.\n",
      "\n",
      "AMD Ryzen 9000 \"Zen 5\" Desktop CPUs Prices For China Appear To Be Much Lower Than Ryzen 7000 MSRPs\n",
      "\n",
      "80IT, a computer hardware store based in Bejing China, and affiliated with Taobao, has put up what seems to be their expected prices for China. The shop owner also showed off Ryzen 9000 Desktop CPU boxes which suggests that they might be the ones to receive the early batch of Zen 5 chips but we know now that they have been recalled due to quality assurance issues. (The live stream is still going on and do check it out if you want a chuckle).\n",
      "\n",
      "Image Source: 80IT\n",
      "\n",
      "So what makes this price exposure weird is that the shopkeeper has casually put up a board that shows the initial prices in USD and Chinese RMB currencies. Previously, the same shopkeeper also showed off boxes of the chip behind the board which listed estimated prices but they have since put up what they believe are \"initial\" prices for the Ryzen 9000 \"Zen 5\" Desktop CPUs.\n",
      "\n",
      "Image Source: 80IT\n",
      "\n",
      "According to the retailer, the AMD Ryzen 9 9950X 16-Core CPU would cost $499 US (3631 RMB), the Ryzen 9 9900X 12-Core CPU would cost $399 US (2904 RMB), the Ryzen 7 9700X 8-Core would cost $299 US (2176 RMB) while the Ryzen 5 9600X 6-Core CPU would cost $229 US (1666 RMB). Previous \"estimated\" listing had the prices at 5099 RMB, 3799 RMB, 3099 RMB, and 2399 RMB for respective Ryzen 9000 \"Zen 5\" CPUs.\n",
      "\n",
      "Following are the official launch MSRPs of AMD's existing Ryzen 7000 \"Zen 4\" Desktop CPUs that can be used for comparison:\n",
      "\n",
      "Ryzen 9 7950X - 5499 RMB\n",
      "\n",
      "5499 RMB Ryzen 9 7900X - 4299 RMB\n",
      "\n",
      "4299 RMB Ryzen 7 7700X - 2999 RMB\n",
      "\n",
      "2999 RMB Ryzen 5 7600X - 2249 RMB\n",
      "\n",
      "Now it is entirely possible that the shopkeeper may have seen the recent pricing rumors and based the pricing off of those but it's still interesting nonetheless since it's coming from a shopkeeper that has the AMD Ryzen 9000 \"Zen 5\" Desktop CPUs with them.\n",
      "\n",
      "In addition to the prices, the shopkeeper also shares some of his personal performance metrics & compares the Ryzen 9 9950X, Ryzen 7 9700X & Ryzen 5 9600X against Intel's 14th Gen offerings. He states that the Ryzen 9 9950X offers lower power consumption, lower temperatures, and higher multi-threading performance versus the 14900K with an average 20% performance increase in a wide variety of applications. Without PBO, the gaming performance is said to be 15% ahead of the 14900K.\n",
      "\n",
      "Original Non-Translated Image:\n",
      "\n",
      "Translated Image:\n",
      "\n",
      "Image Source: 80IT\n",
      "\n",
      "For the AMD Ryzen 7 9700X, the CPU offers an average 70W power consumption in gaming and runs at around 60-65C using a standard 6-heatpipe air cooler. With PBO, the Ryzen 7 9700X can beat the Ryzen 7 7800X3D and its single-core performance is also higher versus the Intel Core i7-14700K. Lastly, the Ryzen 5 9600X is said to lead over the Core i5-14600K by 10-20% in games and can even outperform the Ryzen 9 9950X in certain titles.\n",
      "\n",
      "The AMD Ryzen 9000 \"Zen 5\" Desktop CPUs will now be launching in August. The Ryzen 7 9700X & Ryzen 5 9600X will be hitting retail shelves on the 8th of August while the Ryzen 9 9950X & the Ryzen 9 9900X will be hitting retail shelves on the 15th of August. Expect a proper pricing update close to the launch.\n",
      "\n",
      "AMD Ryzen 9000 \"Zen 5\" Desktop CPUs Prices For China (Rumor):\n",
      "\n",
      "CPU Name Predecessor's MSRP (Launch) 80IT's Estimated Ryzen 9000 Pricing 80IT's Initial Ryzen 9000 Pricing Ryzen 9 9950X 5499 RMB ($760) 5099 RMB ($705) 3631 RMB ($499) Ryzen 9 9900X 4299 RMB ($594) 3799 RMB ($525) 2904 RMB ($399) Ryzen 7 9700X 2999 RMB ($414) 3099 RMB ($428) 2176 RMB ($299) Ryzen 5 9600X 2249 RMB ($311) 2399 RMB ($331) 1666 RMB ($229)\n"
     ]
    }
   ],
   "source": [
    "article = news_articles[32]\n",
    "print(f\"Title: {article['title']}\")\n",
    "print(f\"Date: {article['published_at']}\")\n",
    "print(f\"Source: {article['media_source_name']}\")\n",
    "text: str = article[\"full_text\"]\n",
    "print()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities: ['Ryzen', 'MSRPs', 'Core', 'Taobao', 'CPU', 'AMD', 'RMB', 'Chinese', 'Intels', 'CPUs', 'Bejing', 'US', 'AMDs', 'USD', 'Desktop', 'August', 'Gen', 'PBO', 'China', 'Zen', 'Intel']\n"
     ]
    }
   ],
   "source": [
    "# Remove some punctuation and split into sentences\n",
    "clean_text = re.sub(r\"[^\\w\\s.]\", \"\", text)\n",
    "clean_text = re.sub(r\"\\n\", \".\", clean_text)\n",
    "sentences = clean_text.split(\".\")\n",
    "sentences = [sentence.strip() for sentence in sentences if sentence]\n",
    "\n",
    "# Remove sentences with all capitalized words\n",
    "sentence_words = [sentence.split() for sentence in sentences]\n",
    "sentence_words = [words for words in sentence_words if any(word[0].islower() for word in words)]\n",
    "\n",
    "# Remove first word of each sentence\n",
    "sentence_words = [words[1:] for words in sentence_words]\n",
    "\n",
    "# Flatten list\n",
    "words = [word for sentence in sentence_words for word in sentence]\n",
    "\n",
    "# Select capitalized words\n",
    "entities = [word for word in words if word[0].isupper()]\n",
    "entities = list(set(entities))\n",
    "\n",
    "print(\"entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy\n",
    "\n",
    "https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities (en_core_web_sm):\n",
      "  ORG: {'Core', 'Original Non-Translated Image', 'Taobao', 'Initial Ryzen 9000 Pricing Ryzen', 'CPU', '8-Core', 'AMD Ryzen 9000', 'the Ryzen 9000', 'the AMD Ryzen', 'AMD', 'the Ryzen 7', 'the AMD Ryzen 9000', 'Ryzen 7', 'the Ryzen 7 9700X', '9700X & Ryzen', 'The AMD Ryzen 9000', 'the Intel Core i7-14700K. Lastly', 'PBO', 'Ryzen 9000 Desktop', 'Intel'}\n",
      "  GPE: {'Bejing', 'China'}\n",
      "  PERSON: {'Ryzen'}\n",
      "\n",
      "entities (en_core_web_trf):\n",
      "  ORG: {'Taobao', 'Intel', '80IT', 'AMD'}\n",
      "  GPE: {'Bejing', 'China'}\n",
      "  PERSON: {'80IT'}\n"
     ]
    }
   ],
   "source": [
    "def get_spacy_entities(text: str, spacy_model_name: str = \"en_core_web_sm\") -> dict[str, list[str]]:\n",
    "    spacy_nlp = spacy.load(spacy_model_name)\n",
    "    doc = spacy_nlp(text)\n",
    "    entities = {}\n",
    "    for ent_type in {\"PERSON\", \"GPE\", \"ORG\"}:\n",
    "        entities[ent_type] = set([ent.text for ent in doc.ents if ent.label_ == ent_type])\n",
    "    return entities\n",
    "\n",
    "\n",
    "entities = get_spacy_entities(text, spacy_model_name=\"en_core_web_sm\")\n",
    "print(\"entities (en_core_web_sm):\")\n",
    "for k, v in entities.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# print()\n",
    "# entities = get_spacy_entities(text, spacy_model_name=\"en_core_web_trf\")\n",
    "# print(\"entities (en_core_web_trf):\")\n",
    "# for k, v in entities.items():\n",
    "#     print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT\n",
    "\n",
    "https://platform.openai.com/docs/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organizations: ['AMD', '80IT', 'Taobao']\n",
      "locations: ['Bejing', 'China']\n",
      "people: []\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "query = (\n",
    "    \"\"\"Return a JSON of with a list of named entities from the_text.\n",
    "    Output JSON: {\n",
    "        organizations: ['organization_1', 'organization_2', ...],\n",
    "        locations: ['location_1', 'location_2', ...],\n",
    "        people: ['person_1', 'person_2', ...]\n",
    "    }\n",
    "\n",
    "    The_text:\\n\\n\n",
    "    \"\"\"\n",
    "    + text\n",
    ")\n",
    "gpt_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"named entity extractor\"},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "openai_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=gpt_messages,  # type: ignore\n",
    "    stream=False,\n",
    "    max_tokens=256,\n",
    "    n=1,\n",
    "    frequency_penalty=0,\n",
    "    temperature=0.5,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "response_text = openai_response.choices[0].message.content\n",
    "json_obj = json.loads(response_text)\n",
    "\n",
    "print(\"entities:\")\n",
    "for k, v in json_obj.items():\n",
    "    print(f\" {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword extraction\n",
    "\n",
    "Let's extract meaningful keywords from text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "taylor swift also revealed that shes godmother to blake lively and ryan reynolds kidsblake lively is matching taylor swifts energythe 36yearold actress had a hilarious response to swifts recent instag\n",
      "------\n",
      "\n",
      "most common: [('and', 9), ('the', 8), ('reynolds', 6), ('to', 5), ('swifts', 4), ('a', 4), ('his', 4), ('lively', 3)]\n",
      "most common: [('and', 9), ('the', 8), ('reynolds', 6), ('swifts', 4), ('his', 4), ('lively', 3), ('film', 3), ('taylor', 2)]\n",
      "most common: [('reynolds', 6), ('swifts', 4), ('lively', 3), ('film', 3), ('taylor', 2), ('swift', 2), ('story', 2), ('tribute', 2)]\n",
      "most common: [('swift', 6), ('reynolds', 6), ('lively', 5), ('film', 3), ('taylor', 2), ('story', 2), ('tribute', 2), ('work', 2)]\n",
      "keywords: [('swift', 6), ('reynolds', 6), ('lively', 5), ('film', 3), ('taylor', 2), ('story', 2), ('tribute', 2), ('work', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "clean_text = re.sub(r\"[^\\w ]\", \"\", text.lower())\n",
    "print(\"------\")\n",
    "print(clean_text[:200])\n",
    "print(\"------\\n\")\n",
    "words = clean_text.split()\n",
    "word_counts = Counter(words)\n",
    "print(\"most common:\", word_counts.most_common(8))\n",
    "\n",
    "# Remove short words\n",
    "words = [word for word in words if len(word) > 2]\n",
    "word_counts = Counter(words)\n",
    "print(\"most common:\", word_counts.most_common(8))\n",
    "\n",
    "# Remove pronouns, prepositions, articles, and simple verbs\n",
    "exclude_words = set(\n",
    "    [\"a\", \"for\", \"in\", \"the\", \"and\", \"to\", \"of\"]\n",
    "    + [\"can\", \"do\", \"does\", \"did\", \"have\", \"has\", \"had\", \"was\", \"is\", \"are\", \"were\"]\n",
    "    + [\"it\", \"i\", \"you\", \"he\", \"she\", \"her\", \"his\", \"that\", \"this\"]\n",
    ")\n",
    "words = [word for word in words if word not in exclude_words]\n",
    "word_counts = Counter(words)\n",
    "print(\"most common:\", word_counts.most_common(8))\n",
    "\n",
    "# Plural to singular\n",
    "unique_words = set(words)\n",
    "words = [word[:-1] if word.endswith(\"s\") and word[:-1] in unique_words else word for word in words]\n",
    "word_counts = Counter(words)\n",
    "print(\"most common:\", word_counts.most_common(8))\n",
    "\n",
    "keywords = list(word_counts.most_common(8))\n",
    "print(\"keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy\n",
    "\n",
    "CNN or Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords (en_core_web_sm): [('swift', 7), ('reynolds', 6), ('lively', 3), ('film', 3), ('taylor', 2), ('story', 2), ('tribute', 2), ('work', 2)]\n",
      "keywords (en_core_web_trf): [('swift', 7), ('reynolds', 6), ('lively', 5), ('film', 3), ('taylor', 2), ('blake', 2), ('story', 2), ('tribute', 2)]\n"
     ]
    }
   ],
   "source": [
    "def get_spacy_keywords(\n",
    "    text: str, n: int = 10, spacy_model_name: str = \"en_core_web_sm\"\n",
    ") -> list[tuple[str, int]]:\n",
    "    spacy_nlp = spacy.load(spacy_model_name)\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    pos_tag = [\"PROPN\", \"ADJ\", \"NOUN\"]  # only include proper nouns, adjectives, and nouns\n",
    "    hotwords = [token.text.lower() for token in doc if token.pos_ in pos_tag]\n",
    "    keywords = list(Counter(hotwords).most_common(n))\n",
    "    return keywords\n",
    "\n",
    "\n",
    "keywords = get_spacy_keywords(text, n=8, spacy_model_name=\"en_core_web_sm\")\n",
    "print(\"keywords (en_core_web_sm):\", keywords)\n",
    "\n",
    "# keywords = get_spacy_keywords(text, n=8, spacy_model_name=\"en_core_web_trf\")\n",
    "# print(\"keywords (en_core_web_trf):\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy + Pytextrank\n",
    "\n",
    "Graph-based ranking algorithm inspired by Google's PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords (en_core_web_sm): [('hugh jackman', 0.14), ('james', 0.11), ('taylor', 0.09), ('james\"', 0.05), (',\\n    \"instagram story', 0.04), (',\\n    \"wade wilson', 0.04), (',\\n    \"hugh jackman', 0.04), (',\\n    \"marvel film', 0.04)]\n",
      "keywords (en_core_web_trf): [('blake lively', 0.14), ('ryan reynolds', 0.14), ('wade wilson', 0.13), ('hugh jackman', 0.13), ('marvel film', 0.13), ('world tour', 0.13), ('james', 0.1), ('kids', 0.1)]\n"
     ]
    }
   ],
   "source": [
    "def get_textrank_keywords(\n",
    "    text: str, n: int = 10, spacy_model_name: str = \"en_core_web_sm\"\n",
    ") -> list[tuple[str, float]]:\n",
    "    spacy_nlp = spacy.load(spacy_model_name)\n",
    "    spacy_nlp.add_pipe(\"textrank\")\n",
    "    doc = spacy_nlp(text.lower())\n",
    "    keywords = [(phrase.text, round(phrase.rank, 2)) for phrase in doc._.phrases[:n]]\n",
    "    return keywords\n",
    "\n",
    "keywords = get_textrank_keywords(text, n=8, spacy_model_name=\"en_core_web_sm\")\n",
    "print(\"keywords (en_core_web_sm):\", keywords)\n",
    "\n",
    "# keywords = get_textrank_keywords(text, n=8, spacy_model_name=\"en_core_web_trf\")\n",
    "# print(\"keywords (en_core_web_trf):\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBERT\n",
    "\n",
    "BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords (KeyBERT): [('deadpool', 0.505), ('swift', 0.4517), ('marvel', 0.4111), ('reynolds', 0.409), ('taylor', 0.3546)]\n"
     ]
    }
   ],
   "source": [
    "kw_model = keybert.KeyBERT()\n",
    "keywords = kw_model.extract_keywords(text)\n",
    "print(\"keywords (KeyBERT):\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keywords: ['taylor swift', 'blake lively', 'ryan reynolds', 'instagram story', 'deadpool', 'wolverine', 'grammys', 'world tour', 'marvel film', 'hugh jackman', 'wade wilson', 'goddaughter', 'kids', 'james', 'inez', 'betty', 'olin']\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI()\n",
    "query = (\n",
    "    \"Return a list of keywords from the_text. Output JSON: {keywords: ['keyword_1', 'keyword_2', ...]}. The_text:\\n\\n\"\n",
    "    + text\n",
    ")\n",
    "gpt_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"keyword extractor\"},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "openai_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=gpt_messages,  # type: ignore\n",
    "    stream=False,\n",
    "    max_tokens=256,\n",
    "    n=1,\n",
    "    frequency_penalty=0,\n",
    "    temperature=0.5,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "response_text = openai_response.choices[0].message.content\n",
    "json_obj = json.loads(response_text)\n",
    "keywords = json_obj[\"keywords\"]\n",
    "keywords = [keyword.lower() for keyword in keywords]\n",
    "print(\"keywords:\", keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
